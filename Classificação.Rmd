---
title: "Classificação"
author: "Wllyssys"
date: "27/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## KNN | K-vizinhos mais próximos | K nearest neighboors

### Introdução à Ciência de Dados - Fernando Amaral

- funciona apenas com atrubutos númericos
- a classe deve ser nominal

```{r cars}

library(class)
amostra = sample(2, 150, replace = T, prob = c(0.7 , 0.3))
amostra
iristreino = iris[amostra==1,]
iristreino
classificar = iris[amostra==2,]
classificar
# passando o iristreino todas as linhas as quatro colunas
# passando o classificar todas as linhas as quatro colunas
# passsando os níveis de classificação
#
previsao = knn(iristreino[,1:4] , classificar[,1:4] , iristreino[,5], k = 1)
previsao

# a quinta coluna estava com a mesma classificação iris, então iremos compará-las
# com a da previsão pra vermos a precisão da previsão
confusao = table(classificar[,5] , previsao)
confusao

taxa_acerto = (confusao[1,1] + confusao[2,2] + confusao[3,3])/sum(confusao)
taxa_acerto


taxa_erro = (sum(confusao)-(confusao[1,1] + confusao[2,2] + confusao[3,3]))/sum(confusao)
taxa_erro
```

### Computação Inteligente

#### Distâncias utilizadas

Fonte: http://computacaointeligente.com.br/algoritmos/k-vizinhos-mais-proximos/

1. Euclidiana
2. Minkowsky
3. Chebyshev

#### A escolha de K

Em relação a escolha do valor k, não existe um valor único para a constante, a mesma varia de acordo com a base de dados. É recomendável sempre utilizar valores ímpares/primos, mas o valor ótimo varia de acordo com a base de dados. Dependendo do seu problema, você pode utilizar um algoritmo de otimização (PSO, GA, DE etc) para encontrar o melhor valor para o seu problema. Todavia, você pode deixar o desempenho geral do modelo bem lento na etapa de seleção de k. Outra maneira e simplesmente testar um conjunto de valores e encontrar o valor de k empiricamente.

